{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf83439-16a6-4fc6-b88a-b321647ab6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eead76d-96c6-437a-b569-9404d2e0f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Min-Max scaling**, also known as normalization, is a data preprocessing technique used to scale numeric features within a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to bring all features to a common scale, making them directly comparable and preventing features with larger magnitudes from dominating those with smaller magnitudes during model training. It is especially important for machine learning algorithms that are sensitive to the scale of input features, such as K-means clustering or gradient-based optimization methods.\n",
    "\n",
    "The formula for Min-Max scaling is as follows for each feature:\n",
    "\n",
    "\\[X_{\\text{new}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{new}}\\) is the scaled value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of the feature in the dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "Here's an example to illustrate how Min-Max scaling works:\n",
    "\n",
    "**Scenario:** Suppose you have a dataset containing the ages of individuals and their corresponding income levels. The age values range from 18 to 65, while the income values range from $20,000 to $100,000.\n",
    "\n",
    "**Before Min-Max Scaling:**\n",
    "- Age (in years): [18, 22, 35, 48, 65]\n",
    "- Income (in dollars): [$20,000, $30,000, $50,000, $70,000, $100,000]\n",
    "\n",
    "**After Min-Max Scaling:**\n",
    "- Age (scaled): [0.0, 0.2143, 0.5714, 0.9286, 1.0]\n",
    "- Income (scaled): [0.0, 0.1111, 0.3333, 0.5556, 1.0]\n",
    "\n",
    "In this example, Min-Max scaling transforms both age and income features to a range between 0 and 1. For instance, an age of 35 years gets scaled to approximately 0.5714, while an income of $50,000 gets scaled to approximately 0.3333.\n",
    "\n",
    "Min-Max scaling ensures that the data maintains its relative relationships, so the order of values is preserved. It can be beneficial when using machine learning algorithms that rely on distance metrics or when features have different units or magnitudes. However, it may not be suitable for data with outliers because extreme values can distort the scaling. In such cases, robust scaling techniques like Z-score scaling (standardization) may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d4aed-7213-452e-a36d-d73f2596eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a0347-3a62-4044-b9a4-1fc977809fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Unit Vector scaling**, also known as vector normalization, is a feature scaling technique used to transform data such that each data point (vector) has a length or magnitude of 1. This technique is particularly useful when you want to maintain the direction of vectors while scaling them. It's often applied in machine learning contexts, especially for algorithms that rely on vector operations, such as cosine similarity in text mining or collaborative filtering.\n",
    "\n",
    "The formula for unit vector scaling is as follows:\n",
    "\n",
    "\\[X_{\\text{new}} = \\frac{X}{\\|X\\|}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{new}}\\) is the unit vector of the original feature vector \\(X\\).\n",
    "- \\(X\\) is the original feature vector.\n",
    "- \\(\\|X\\|\\) represents the Euclidean norm or magnitude of the vector \\(X\\), which is calculated as the square root of the sum of squared values.\n",
    "\n",
    "Here's an example to illustrate how unit vector scaling works:\n",
    "\n",
    "**Scenario:** Suppose you have a dataset with two features representing coordinates in 2D space (x and y). You want to scale these feature vectors to unit vectors.\n",
    "\n",
    "**Before Unit Vector Scaling:**\n",
    "- Feature vector 1 (x, y): (3, 4)\n",
    "- Feature vector 2 (x, y): (-2, 5)\n",
    "\n",
    "**After Unit Vector Scaling:**\n",
    "- Unit vector 1 (scaled): \\(\\left(\\frac{3}{5}, \\frac{4}{5}\\right)\\)\n",
    "- Unit vector 2 (scaled): \\(\\left(\\frac{-2}{\\sqrt{29}}, \\frac{5}{\\sqrt{29}}\\right)\\)\n",
    "\n",
    "In this example, the original feature vectors (3, 4) and (-2, 5) are scaled to unit vectors with lengths of 1. The first unit vector maintains the direction of the original vector, while the second unit vector also preserves the direction but with a magnitude of 1.\n",
    "\n",
    "**Differences from Min-Max Scaling:**\n",
    "\n",
    "1. **Purpose:** Min-Max scaling is primarily used to scale numeric features to a specific range (usually between 0 and 1), preserving the relative relationships between data points. Unit vector scaling is used to transform data points into unit vectors, maintaining their direction while making their magnitude 1.\n",
    "\n",
    "2. **Magnitude:** Min-Max scaling changes the magnitude of data points to fit within a specified range. Unit vector scaling keeps the magnitude constant at 1.\n",
    "\n",
    "3. **Application:** Min-Max scaling is commonly used for algorithms sensitive to feature scales, whereas unit vector scaling is used in scenarios where the direction or angle between vectors is important, such as in cosine similarity calculations.\n",
    "\n",
    "4. **Data Type:** Min-Max scaling is typically applied to numeric features. Unit vector scaling can be applied to any feature that can be represented as a vector, including text data for natural language processing.\n",
    "\n",
    "In summary, unit vector scaling is a technique for transforming feature vectors into unit vectors, preserving their direction. It differs from Min-Max scaling, which changes the magnitude of data points to fit within a specified range. The choice between these techniques depends on the specific requirements of your machine learning task and the nature of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1df92-6c6d-4d38-b791-05439026fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa268b6-7a1c-4670-a273-bd0ca5bd0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique commonly used in data analysis and machine learning. Its primary objective is to reduce the dimensionality of a dataset while retaining as much of the original variability (information) as possible. PCA achieves this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. **Standardization:** If the features in the dataset have different scales, it's important to standardize them (e.g., by subtracting the mean and dividing by the standard deviation) so that all features are on the same scale.\n",
    "\n",
    "2. **Covariance Matrix:** PCA calculates the covariance matrix of the standardized dataset. The covariance matrix describes the relationships between pairs of features, including their linear dependencies.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** PCA then performs eigenvalue decomposition (or singular value decomposition) on the covariance matrix to find its eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Selection of Principal Components:** The eigenvectors represent the directions (principal components) along which the data varies the most, while the eigenvalues indicate the magnitude of variance explained by each principal component. PCA sorts the eigenvectors by eigenvalue magnitude in descending order.\n",
    "\n",
    "5. **Dimension Reduction:** To reduce the dimensionality, you can select the top \\(k\\) principal components that capture the most variance. This effectively reduces the dataset from \\(n\\) features to \\(k\\) features, where \\(k < n\\).\n",
    "\n",
    "6. **Projection:** The original data is then projected onto the selected principal components to obtain a reduced-dimensional representation of the data.\n",
    "\n",
    "**Example:**\n",
    "Suppose you have a dataset with three features: height (in inches), weight (in pounds), and age (in years) of individuals. You want to perform PCA for dimensionality reduction.\n",
    "\n",
    "**Original Dataset:**\n",
    "- Height (in inches): [63, 67, 70, 64, 72]\n",
    "- Weight (in pounds): [120, 140, 160, 135, 180]\n",
    "- Age (in years): [25, 30, 35, 28, 40]\n",
    "\n",
    "**PCA Steps:**\n",
    "1. Standardize the data (subtract mean and divide by standard deviation).\n",
    "2. Calculate the covariance matrix.\n",
    "3. Perform eigenvalue decomposition to find eigenvectors and eigenvalues.\n",
    "4. Sort eigenvectors by eigenvalue magnitude:\n",
    "\n",
    "   Eigenvector 1: [0.57, 0.59, 0.57]\n",
    "   Eigenvector 2: [0.64, -0.33, -0.68]\n",
    "   Eigenvector 3: [-0.52, -0.74, 0.41]\n",
    "\n",
    "5. Choose the top \\(k\\) principal components. Let's say you select the top two.\n",
    "6. Project the data onto the two selected principal components.\n",
    "\n",
    "The resulting dataset is now two-dimensional, capturing most of the variability in the original data. This reduction in dimensionality can be beneficial for visualization, computational efficiency, and sometimes improving model performance, especially when dealing with high-dimensional datasets.\n",
    "\n",
    "PCA is widely used in various fields, including image processing, pattern recognition, and data compression, to uncover the most important features and reduce the computational burden associated with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0cccc-6bc7-4cba-b9bf-e2dd186aaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaaf199-1137-4c57-b873-cf7b76654c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "**PCA (Principal Component Analysis)** is closely related to feature extraction, and it can be used as a feature extraction technique. Feature extraction aims to transform the original features of a dataset into a new set of features while retaining relevant information and reducing dimensionality. PCA achieves this by creating a set of linearly uncorrelated features called principal components, which can be thought of as a new representation of the data.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "**Step 1: Standardization of Data**\n",
    "- Start by standardizing the original data if necessary, ensuring that all features have the same scale. Standardization subtracts the mean and divides by the standard deviation for each feature.\n",
    "\n",
    "**Step 2: Principal Component Analysis**\n",
    "- Perform PCA on the standardized data:\n",
    "  1. Calculate the covariance matrix of the data.\n",
    "  2. Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "  3. Sort the eigenvectors by the magnitude of their corresponding eigenvalues in descending order.\n",
    "\n",
    "**Step 3: Feature Selection**\n",
    "- Select a subset of the top \\(k\\) principal components to use as the new features. The number \\(k\\) is determined based on how many principal components you want to retain. You can choose \\(k\\) to reduce dimensionality or retain a certain amount of variance.\n",
    "\n",
    "**Step 4: Feature Extraction**\n",
    "- Project the original data onto the selected \\(k\\) principal components. This projection generates the new feature set, where each feature represents a linear combination of the original features.\n",
    "\n",
    "**Example:**\n",
    "Let's illustrate PCA as a feature extraction technique using a dataset of handwritten digits, specifically focusing on the dimensionality reduction aspect:\n",
    "\n",
    "**Original Dataset:** Imagine you have a dataset of handwritten digits (0 to 9), each represented as a 28x28 pixel image, resulting in 784 features per image.\n",
    "\n",
    "**PCA for Feature Extraction:**\n",
    "1. Standardize the pixel values across all images.\n",
    "2. Perform PCA on the standardized data.\n",
    "3. Suppose you choose to retain the top 50 principal components for dimensionality reduction.\n",
    "\n",
    "   Result:\n",
    "   - You now have a new feature set with 50 features.\n",
    "   - These 50 features are linear combinations of the original 784 pixel values.\n",
    "   - Each feature represents a unique direction in the pixel space that captures the most significant variation in the data.\n",
    "\n",
    "This transformed feature set with reduced dimensionality can be used as input for machine learning algorithms or for visualization purposes. The key advantage is that you've captured the most relevant information in a much lower-dimensional space, making it computationally efficient and potentially improving the performance of machine learning models, especially when dealing with high-dimensional data.\n",
    "\n",
    "In summary, PCA can be used for feature extraction by creating a new feature set that retains important information while reducing dimensionality. It is particularly useful when you want to simplify complex datasets, improve model training efficiency, or perform data visualization in a lower-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364b8af-0756-4cfa-b3de-a4103c34921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050425aa-0a04-46c9-84a4-f4dd86aa57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "To preprocess the dataset for building a recommendation system for a food delivery service, which includes features like price, rating, and delivery time, you can use **Min-Max scaling** to ensure that all the features are on the same scale within a specified range, typically between 0 and 1. Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "**Step 1: Data Collection and Cleaning:**\n",
    "- Collect the dataset containing features such as price, rating, and delivery time.\n",
    "- Perform data cleaning, handling missing values, outliers, and any other data quality issues.\n",
    "\n",
    "**Step 2: Feature Selection:**\n",
    "- Determine which features you want to include in your recommendation system. In this case, you mentioned price, rating, and delivery time, which seem relevant for making recommendations.\n",
    "\n",
    "**Step 3: Min-Max Scaling:**\n",
    "- Apply Min-Max scaling to each of the selected features separately. The goal is to transform the values of each feature so that they fall within the range [0, 1].\n",
    "\n",
    "**Mathematical Formulation of Min-Max Scaling:**\n",
    "For each feature \\(X\\), use the following formula to perform Min-Max scaling:\n",
    "\n",
    "\\[X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{scaled}}\\) is the scaled value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of the feature in the dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "**Step 4: Resulting Scaled Features:**\n",
    "- After applying Min-Max scaling, you will have new versions of the features, which are scaled between 0 and 1. These scaled features will now have values that are directly comparable and won't be biased by their original scales.\n",
    "\n",
    "**Step 5: Building the Recommendation System:**\n",
    "- Use the scaled features as inputs for building your recommendation system. You can employ various techniques, such as collaborative filtering, content-based filtering, or hybrid approaches, to make recommendations based on the scaled features.\n",
    "\n",
    "**Benefits of Min-Max Scaling in this Context:**\n",
    "- By scaling the features, you ensure that no single feature dominates the recommendation process due to differences in scales.\n",
    "- Min-Max scaling helps in improving the stability and performance of recommendation algorithms, particularly when they rely on distance-based calculations.\n",
    "- It allows you to effectively compare and combine different types of features (e.g., price and rating) without any scaling bias.\n",
    "\n",
    "In summary, Min-Max scaling is an essential preprocessing step when building a recommendation system that uses features like price, rating, and delivery time. It ensures that these features are on a consistent scale, enabling meaningful and unbiased recommendations for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7889b-f04c-415c-ab4b-311ac0254d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065a10c-c1cb-4cfc-9441-dc26efe85efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices is a common practice, especially when dealing with a dataset that includes a large number of features, such as company financial data and market trends. Reducing dimensionality can help improve model training efficiency, mitigate the curse of dimensionality, and potentially enhance the performance of your stock price prediction model. Here's how you can use PCA for dimensionality reduction in this context:\n",
    "\n",
    "**Step 1: Data Collection and Cleaning:**\n",
    "- Gather the dataset containing features like company financial data (e.g., revenue, earnings, debt) and market trends (e.g., trading volume, sector-specific indicators).\n",
    "- Perform data cleaning, handling missing values, outliers, and any other data quality issues.\n",
    "\n",
    "**Step 2: Feature Selection:**\n",
    "- Decide on which features to include in your stock price prediction model. This selection should be based on domain knowledge and the relevance of features to stock price movements. Initially, include all potentially relevant features.\n",
    "\n",
    "**Step 3: Standardization:**\n",
    "- Standardize the selected features by subtracting the mean and dividing by the standard deviation. Standardization is crucial for PCA to work effectively, especially when features are measured on different scales.\n",
    "\n",
    "**Step 4: PCA Dimensionality Reduction:**\n",
    "- Perform PCA on the standardized dataset. Here are the steps involved:\n",
    "\n",
    "   a. Calculate the covariance matrix of the standardized data. The covariance matrix describes the relationships between pairs of features.\n",
    "\n",
    "   b. Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions (principal components), and eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "   c. Sort the eigenvectors by the magnitude of their corresponding eigenvalues in descending order.\n",
    "\n",
    "   d. Select a subset of the top \\(k\\) principal components to retain. You can choose \\(k\\) based on the desired level of dimensionality reduction. Common choices include retaining enough components to explain a certain percentage of variance (e.g., 95% of the variance).\n",
    "\n",
    "   e. Project the original data onto the selected \\(k\\) principal components to obtain a lower-dimensional representation of the dataset.\n",
    "\n",
    "**Step 5: Building the Stock Price Prediction Model:**\n",
    "- Use the reduced-dimensional dataset, which now contains only the selected principal components, as input for building your stock price prediction model.\n",
    "- Employ appropriate machine learning techniques such as regression, time series analysis, or neural networks to create the prediction model.\n",
    "\n",
    "**Benefits of PCA in this Context:**\n",
    "- **Dimensionality Reduction:** PCA reduces the number of features while retaining as much variance as possible, making it easier to work with high-dimensional datasets.\n",
    "- **Noise Reduction:** PCA can help remove noise and redundancy in the data, which is particularly beneficial when dealing with financial and market data that may contain multicollinearity.\n",
    "- **Efficient Modeling:** A reduced feature set can lead to faster model training, reduced memory usage, and improved computational efficiency.\n",
    "- **Mitigating Overfitting:** Dimensionality reduction can help mitigate the risk of overfitting, especially when the model has limited data.\n",
    "\n",
    "In summary, PCA is a valuable technique for reducing the dimensionality of a dataset containing financial and market data for stock price prediction. By selecting and projecting onto a subset of principal components, you can create a more manageable and potentially more effective dataset for building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dffadc-b1bd-499f-abd3-f00d208ce039",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609776b-ed21-408a-8e4b-a5ff09d37e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "To perform Min-Max scaling and transform the values in the dataset to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "**Step 1: Calculate the Minimum and Maximum Values:**\n",
    "- Find the minimum and maximum values in the dataset.\n",
    "\n",
    "**Step 2: Apply the Min-Max Scaling Formula:**\n",
    "- Use the Min-Max scaling formula for each value in the dataset:\n",
    "\n",
    "\\[X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}(X_{\\text{new\\_max}} - X_{\\text{new\\_min}}) + X_{\\text{new\\_min}}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{scaled}}\\) is the scaled value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value in the original dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value in the original dataset.\n",
    "- \\(X_{\\text{new\\_min}}\\) is the new minimum value (-1 in this case).\n",
    "- \\(X_{\\text{new\\_max}}\\) is the new maximum value (1 in this case).\n",
    "\n",
    "**Step 3: Apply Min-Max Scaling to Each Value:**\n",
    "- Use the formula to calculate the scaled values for each value in the dataset.\n",
    "\n",
    "Let's perform the calculations:\n",
    "\n",
    "- Original Dataset: [1, 5, 10, 15, 20]\n",
    "- \\(X_{\\text{min}} = 1\\) (minimum value)\n",
    "- \\(X_{\\text{max}} = 20\\) (maximum value)\n",
    "- \\(X_{\\text{new\\_min}} = -1\\) (new minimum value)\n",
    "- \\(X_{\\text{new\\_max}} = 1\\) (new maximum value)\n",
    "\n",
    "Now, apply the Min-Max scaling formula to each value:\n",
    "\n",
    "1. For \\(X = 1\\):\n",
    "   \\[X_{\\text{scaled}} = \\frac{1 - 1}{20 - 1}(1 - (-1)) + (-1) = 0\\]\n",
    "\n",
    "2. For \\(X = 5\\):\n",
    "   \\[X_{\\text{scaled}} = \\frac{5 - 1}{20 - 1}(1 - (-1)) + (-1) = -0.5\\]\n",
    "\n",
    "3. For \\(X = 10\\):\n",
    "   \\[X_{\\text{scaled}} = \\frac{10 - 1}{20 - 1}(1 - (-1)) + (-1) = -0.15\\]\n",
    "\n",
    "4. For \\(X = 15\\):\n",
    "   \\[X_{\\text{scaled}} = \\frac{15 - 1}{20 - 1}(1 - (-1)) + (-1) = 0.2\\]\n",
    "\n",
    "5. For \\(X = 20\\):\n",
    "   \\[X_{\\text{scaled}} = \\frac{20 - 1}{20 - 1}(1 - (-1)) + (-1) = 1\\]\n",
    "\n",
    "After applying Min-Max scaling, the dataset is transformed to the range of -1 to 1:\n",
    "\n",
    "- Scaled Dataset: [0, -0.5, -0.15, 0.2, 1]\n",
    "\n",
    "Now, all values in the dataset fall within the specified range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b353a-9e9e-46dc-bdce-6531ebef24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346651a-9280-42c6-8105-2cf6926fa80f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
